{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd5b7b63",
   "metadata": {},
   "source": [
    "# CutLER - Entraînement sur Dataset Personnalisé\n",
    "\n",
    "Ce notebook démontre comment utiliser CutLER pour entraîner un modèle de détection d'objets sur votre propre dataset.\n",
    "\n",
    "## Fonctionnalités ajoutées :\n",
    "\n",
    "1. **Génération automatique de datasets synthétiques** - Crée des images avec formes géométriques\n",
    "2. **Support de datasets personnalisés** - Compatible avec le format COCO\n",
    "3. **Scripts d'entraînement adaptés** - Configuration automatique pour votre dataset\n",
    "4. **Outils de visualisation** - Comparaison ground truth vs prédictions\n",
    "5. **Pipeline complet** - De la génération des données à l'évaluation\n",
    "\n",
    "## Structure des fichiers créés :\n",
    "\n",
    "- `tools/generate_custom_dataset.py` - Génère un dataset synthétique\n",
    "- `tools/train_custom.py` - Entraîne le modèle sur votre dataset\n",
    "- `tools/visualize_custom.py` - Visualise les résultats\n",
    "- `tools/run_custom_pipeline.py` - Script principal\n",
    "- `cutler/data/datasets/custom_datasets.py` - Support pour datasets personnalisés\n",
    "- `cutler/model_zoo/configs/Custom-Dataset/` - Configuration pour datasets personnalisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb586eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Ajouter le répertoire du projet au PYTHONPATH\n",
    "project_root = Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Répertoire du projet: {project_root}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Vérifier que les scripts personnalisés existent\n",
    "tools_dir = project_root / \"tools\"\n",
    "custom_scripts = [\n",
    "    \"generate_custom_dataset.py\",\n",
    "    \"train_custom.py\",\n",
    "    \"visualize_custom.py\",\n",
    "    \"run_custom_pipeline.py\"\n",
    "]\n",
    "\n",
    "print(\"\\nScripts personnalisés:\")\n",
    "for script in custom_scripts:\n",
    "    script_path = tools_dir / script\n",
    "    status = \"✓\" if script_path.exists() else \"✗\"\n",
    "    print(f\"{status} {script}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbdfb1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Démonstration 1: Génération d'un dataset personnalisé\n",
    "\n",
    "# Créer un répertoire de test\n",
    "workspace_dir = project_root / \"test_workspace\"\n",
    "workspace_dir.mkdir(exist_ok=True)\n",
    "\n",
    "dataset_dir = workspace_dir / \"custom_dataset\"\n",
    "\n",
    "print(f\"Génération d'un dataset d'exemple dans: {dataset_dir}\")\n",
    "\n",
    "# Commande pour générer le dataset\n",
    "cmd = [\n",
    "    sys.executable,\n",
    "    str(tools_dir / \"generate_custom_dataset.py\"),\n",
    "    \"--output-dir\", str(dataset_dir),\n",
    "    \"--num-train\", \"20\",  # Petit dataset pour test\n",
    "    \"--num-val\", \"5\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "    print(\"✓ Dataset généré avec succès!\")\n",
    "    print(\"\\nStructure créée:\")\n",
    "\n",
    "    # Afficher la structure du dataset\n",
    "    for root, dirs, files in os.walk(dataset_dir):\n",
    "        level = root.replace(str(dataset_dir), '').count(os.sep)\n",
    "        indent = ' ' * 2 * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        subindent = ' ' * 2 * (level + 1)\n",
    "        for file in files[:3]:  # Limiter l'affichage\n",
    "            print(f\"{subindent}{file}\")\n",
    "        if len(files) > 3:\n",
    "            print(f\"{subindent}... et {len(files)-3} autres fichiers\")\n",
    "\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"✗ Erreur: {e}\")\n",
    "    print(f\"Sortie: {e.stdout}\")\n",
    "    print(f\"Erreur: {e.stderr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c5297",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3797704349.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mhuggingface-cli whoami\u001b[39m\n                    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Démonstration 2: Visualisation du dataset généré\n",
    "\n",
    "if dataset_dir.exists():\n",
    "    print(\"Visualisation des annotations ground truth...\")\n",
    "\n",
    "    visualization_dir = workspace_dir / \"visualizations\"\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        str(tools_dir / \"visualize_custom.py\"),\n",
    "        \"--dataset-path\", str(dataset_dir),\n",
    "        \"--output-dir\", str(visualization_dir),\n",
    "        \"--mode\", \"gt\",\n",
    "        \"--num-images\", \"3\",\n",
    "        \"--split\", \"train\"\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            cmd, capture_output=True, text=True, check=True)\n",
    "        print(\"✓ Visualisations créées!\")\n",
    "\n",
    "        # Lister les fichiers de visualisation créés\n",
    "        if visualization_dir.exists():\n",
    "            viz_files = list(visualization_dir.glob(\"*.png\"))\n",
    "            print(f\"\\nFichiers de visualisation créés: {len(viz_files)}\")\n",
    "            for file in viz_files:\n",
    "                print(f\"  - {file.name}\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"✗ Erreur lors de la visualisation: {e}\")\n",
    "        print(f\"Sortie: {e.stdout}\")\n",
    "        print(f\"Erreur: {e.stderr}\")\n",
    "\n",
    "else:\n",
    "    print(\"Dataset non trouvé. Exécutez d'abord la cellule de génération.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6ce521",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Couldn't reach 'cifar10' on the Hub (LocalEntryNotFoundError)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcifar10\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain[:1\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43m]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cutler/lib/python3.11/site-packages/datasets/load.py:2129\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2124\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   2125\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   2126\u001b[39m )\n\u001b[32m   2128\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2129\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2142\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2144\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2146\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   2147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cutler/lib/python3.11/site-packages/datasets/load.py:1849\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[39m\n\u001b[32m   1847\u001b[39m     download_config = download_config.copy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[32m   1848\u001b[39m     download_config.storage_options.update(storage_options)\n\u001b[32m-> \u001b[39m\u001b[32m1849\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1850\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1851\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1852\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1853\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1854\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1855\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1856\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1857\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1859\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1860\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1861\u001b[39m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[32m   1862\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cutler/lib/python3.11/site-packages/datasets/load.py:1731\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[39m\n\u001b[32m   1726\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1727\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1728\u001b[39m                         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1729\u001b[39m                         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1730\u001b[39m                     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1731\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m trust_remote_code:\n\u001b[32m   1733\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1734\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or any data file in the same directory.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1735\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cutler/lib/python3.11/site-packages/datasets/load.py:1618\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[39m\n\u001b[32m   1609\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m LocalEntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1610\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m   1611\u001b[39m         e.__cause__,\n\u001b[32m   1612\u001b[39m         (\n\u001b[32m   (...)\u001b[39m\u001b[32m   1616\u001b[39m         ),\n\u001b[32m   1617\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1618\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt reach \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hub (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1619\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1620\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectionError\u001b[39m: Couldn't reach 'cifar10' on the Hub (LocalEntryNotFoundError)"
     ]
    }
   ],
   "source": [
    "# Démonstration 3: Pipeline complet (génération + entraînement + visualisation)\n",
    "\n",
    "print(\"Exécution du pipeline complet avec le script principal...\")\n",
    "\n",
    "# Créer un nouveau workspace pour le test complet\n",
    "full_workspace = project_root / \"full_test_workspace\"\n",
    "\n",
    "cmd = [\n",
    "    sys.executable,\n",
    "    str(tools_dir / \"run_custom_pipeline.py\"),\n",
    "    \"--workspace\", str(full_workspace),\n",
    "    \"--num-train\", \"10\",  # Très petit pour test rapide\n",
    "    \"--num-val\", \"3\",\n",
    "    \"--num-classes\", \"3\",\n",
    "    \"--skip-training\"  # Ignorer l'entraînement pour ce test (long)\n",
    "]\n",
    "\n",
    "try:\n",
    "    print(\"Commande exécutée:\")\n",
    "    print(\" \".join(cmd))\n",
    "    print(\"\\nExécution en cours...\")\n",
    "\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "    print(\"✓ Pipeline exécuté avec succès!\")\n",
    "    print(\"\\nSortie:\")\n",
    "    print(result.stdout)\n",
    "\n",
    "    # Afficher la structure créée\n",
    "    if full_workspace.exists():\n",
    "        print(f\"\\nStructure créée dans {full_workspace}:\")\n",
    "        for item in full_workspace.iterdir():\n",
    "            if item.is_dir():\n",
    "                print(f\"  📁 {item.name}/\")\n",
    "                for subitem in item.iterdir():\n",
    "                    if subitem.is_dir():\n",
    "                        print(f\"    📁 {subitem.name}/\")\n",
    "                    else:\n",
    "                        print(f\"    📄 {subitem.name}\")\n",
    "            else:\n",
    "                print(f\"  📄 {item.name}\")\n",
    "\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"✗ Erreur: {e}\")\n",
    "    print(f\"Code de retour: {e.returncode}\")\n",
    "    if e.stdout:\n",
    "        print(f\"Sortie standard:\\n{e.stdout}\")\n",
    "    if e.stderr:\n",
    "        print(f\"Erreur standard:\\n{e.stderr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e62988",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instructions pour utiliser votre propre dataset\n",
    "\n",
    "### 1. Préparez votre dataset au format COCO\n",
    "\n",
    "Organisez vos données comme suit :\n",
    "```\n",
    "mon_dataset/\n",
    "├── images/\n",
    "│   ├── train/\n",
    "│   │   ├── image1.jpg\n",
    "│   │   ├── image2.jpg\n",
    "│   │   └── ...\n",
    "│   └── val/\n",
    "│       ├── image1.jpg\n",
    "│       └── ...\n",
    "└── annotations/\n",
    "    ├── instances_train.json\n",
    "    └── instances_val.json\n",
    "```\n",
    "\n",
    "### 2. Entraînez le modèle\n",
    "\n",
    "```bash\n",
    "python tools/train_custom.py \\\n",
    "    --dataset-path /chemin/vers/mon_dataset \\\n",
    "    --output-dir ./output/mon_entrainement \\\n",
    "    --num-classes NOMBRE_DE_CLASSES\n",
    "```\n",
    "\n",
    "### 3. Visualisez les résultats\n",
    "\n",
    "```bash\n",
    "# Visualiser les annotations ground truth\n",
    "python tools/visualize_custom.py \\\n",
    "    --dataset-path /chemin/vers/mon_dataset \\\n",
    "    --output-dir ./visualizations \\\n",
    "    --mode gt\n",
    "\n",
    "# Comparer avec les prédictions du modèle\n",
    "python tools/visualize_custom.py \\\n",
    "    --dataset-path /chemin/vers/mon_dataset \\\n",
    "    --output-dir ./visualizations \\\n",
    "    --mode compare \\\n",
    "    --model-config ./output/mon_entrainement/config.yaml \\\n",
    "    --model-weights ./output/mon_entrainement/model_final.pth\n",
    "```\n",
    "\n",
    "### 4. Utilisation du script tout-en-un\n",
    "\n",
    "```bash\n",
    "python tools/run_custom_pipeline.py \\\n",
    "    --workspace ./mon_workspace \\\n",
    "    --num-train 1000 \\\n",
    "    --num-val 200 \\\n",
    "    --num-classes NOMBRE_DE_CLASSES\n",
    "```\n",
    "\n",
    "### Notes importantes :\n",
    "\n",
    "- **Format COCO** : Vos annotations doivent être au format COCO JSON\n",
    "- **Classes** : Ajustez `--num-classes` selon votre dataset\n",
    "- **GPU** : L'entraînement nécessite une GPU pour des performances optimales\n",
    "- **Durée** : L'entraînement complet peut prendre plusieurs heures selon la taille du dataset\n",
    "\n",
    "### Fichiers modifiés dans le projet :\n",
    "\n",
    "- ✅ **Support de datasets personnalisés** ajouté\n",
    "- ✅ **Scripts d'entraînement** adaptés pour vos données\n",
    "- ✅ **Outils de visualisation** pour analyser les résultats\n",
    "- ✅ **Configuration automatique** du modèle\n",
    "- ✅ **Pipeline complet** de bout en bout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cutler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
